{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Test simple video feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VIDEO FEED\n",
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    cv2.imshow('Mediapipe Feed', frame)\n",
    "    if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Make detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cap = cv2.VideoCapture(0)\n",
    "cap = cv2.VideoCapture('data/50_ways_to_fall.mp4')\n",
    "\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break # use break if capturing a video file, continue if webcam\n",
    "\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) # mediapipe requires RGB\n",
    "        image.flags.writeable = False # helps manage memory\n",
    "\n",
    "        # Make detections\n",
    "        results = pose.process(image)\n",
    "        \n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # Render detections\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "        cv2.imshow('Mediapipe Feed', image)\n",
    "        \n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Determining joints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://google.github.io/mediapipe/images/mobile/pose_tracking_full_body_landmarks.png' style='height:300px'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cap = cv2.VideoCapture(0)\n",
    "cap = cv2.VideoCapture('data/50_ways_to_fall.mp4')\n",
    "\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break # use break if capturing a video file, continue if webcam\n",
    "\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) # mediapipe requires RGB\n",
    "        image.flags.writeable = False # helps manage memory\n",
    "\n",
    "        # Make detections\n",
    "        results = pose.process(image)\n",
    "        \n",
    "        # Extract landmarks\n",
    "        try:\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "            print(landmarks)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # Render detections\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "        cv2.imshow('Mediapipe Feed', image)\n",
    "        \n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module mediapipe.python.solutions.pose in mediapipe.python.solutions:\n",
      "\n",
      "NAME\n",
      "    mediapipe.python.solutions.pose - MediaPipe Pose.\n",
      "\n",
      "CLASSES\n",
      "    enum.IntEnum(builtins.int, enum.Enum)\n",
      "        PoseLandmark\n",
      "    mediapipe.python.solution_base.SolutionBase(builtins.object)\n",
      "        Pose\n",
      "    \n",
      "    class Pose(mediapipe.python.solution_base.SolutionBase)\n",
      "     |  Pose(static_image_mode=False, model_complexity=1, smooth_landmarks=True, enable_segmentation=False, smooth_segmentation=True, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
      "     |  \n",
      "     |  MediaPipe Pose.\n",
      "     |  \n",
      "     |  MediaPipe Pose processes an RGB image and returns pose landmarks on the most\n",
      "     |  prominent person detected.\n",
      "     |  \n",
      "     |  Please refer to https://solutions.mediapipe.dev/pose#python-solution-api for\n",
      "     |  usage examples.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Pose\n",
      "     |      mediapipe.python.solution_base.SolutionBase\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, static_image_mode=False, model_complexity=1, smooth_landmarks=True, enable_segmentation=False, smooth_segmentation=True, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
      "     |      Initializes a MediaPipe Pose object.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        static_image_mode: Whether to treat the input images as a batch of static\n",
      "     |          and possibly unrelated images, or a video stream. See details in\n",
      "     |          https://solutions.mediapipe.dev/pose#static_image_mode.\n",
      "     |        model_complexity: Complexity of the pose landmark model: 0, 1 or 2. See\n",
      "     |          details in https://solutions.mediapipe.dev/pose#model_complexity.\n",
      "     |        smooth_landmarks: Whether to filter landmarks across different input\n",
      "     |          images to reduce jitter. See details in\n",
      "     |          https://solutions.mediapipe.dev/pose#smooth_landmarks.\n",
      "     |        enable_segmentation: Whether to predict segmentation mask. See details in\n",
      "     |          https://solutions.mediapipe.dev/pose#enable_segmentation.\n",
      "     |        smooth_segmentation: Whether to filter segmentation across different input\n",
      "     |          images to reduce jitter. See details in\n",
      "     |          https://solutions.mediapipe.dev/pose#smooth_segmentation.\n",
      "     |        min_detection_confidence: Minimum confidence value ([0.0, 1.0]) for person\n",
      "     |          detection to be considered successful. See details in\n",
      "     |          https://solutions.mediapipe.dev/pose#min_detection_confidence.\n",
      "     |        min_tracking_confidence: Minimum confidence value ([0.0, 1.0]) for the\n",
      "     |          pose landmarks to be considered tracked successfully. See details in\n",
      "     |          https://solutions.mediapipe.dev/pose#min_tracking_confidence.\n",
      "     |  \n",
      "     |  process(self, image: numpy.ndarray) -> <class 'NamedTuple'>\n",
      "     |      Processes an RGB image and returns the pose landmarks on the most prominent person detected.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        image: An RGB image represented as a numpy ndarray.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |        RuntimeError: If the underlying graph throws any error.\n",
      "     |        ValueError: If the input image is not three channel RGB.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        A NamedTuple with fields describing the landmarks on the most prominate\n",
      "     |        person detected:\n",
      "     |          1) \"pose_landmarks\" field that contains the pose landmarks.\n",
      "     |          2) \"pose_world_landmarks\" field that contains the pose landmarks in\n",
      "     |          real-world 3D coordinates that are in meters with the origin at the\n",
      "     |          center between hips.\n",
      "     |          3) \"segmentation_mask\" field that contains the segmentation mask if\n",
      "     |             \"enable_segmentation\" is set to true.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from mediapipe.python.solution_base.SolutionBase:\n",
      "     |  \n",
      "     |  __enter__(self)\n",
      "     |      A \"with\" statement support.\n",
      "     |  \n",
      "     |  __exit__(self, exc_type, exc_val, exc_tb)\n",
      "     |      Closes all the input sources and the graph.\n",
      "     |  \n",
      "     |  close(self) -> None\n",
      "     |      Closes all the input sources and the graph.\n",
      "     |  \n",
      "     |  reset(self) -> None\n",
      "     |      Resets the graph for another run.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from mediapipe.python.solution_base.SolutionBase:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class PoseLandmark(enum.IntEnum)\n",
      "     |  PoseLandmark(value, names=None, *, module=None, qualname=None, type=None, start=1)\n",
      "     |  \n",
      "     |  The 33 pose landmarks.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      PoseLandmark\n",
      "     |      enum.IntEnum\n",
      "     |      builtins.int\n",
      "     |      enum.Enum\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  LEFT_ANKLE = <PoseLandmark.LEFT_ANKLE: 27>\n",
      "     |  \n",
      "     |  LEFT_EAR = <PoseLandmark.LEFT_EAR: 7>\n",
      "     |  \n",
      "     |  LEFT_ELBOW = <PoseLandmark.LEFT_ELBOW: 13>\n",
      "     |  \n",
      "     |  LEFT_EYE = <PoseLandmark.LEFT_EYE: 2>\n",
      "     |  \n",
      "     |  LEFT_EYE_INNER = <PoseLandmark.LEFT_EYE_INNER: 1>\n",
      "     |  \n",
      "     |  LEFT_EYE_OUTER = <PoseLandmark.LEFT_EYE_OUTER: 3>\n",
      "     |  \n",
      "     |  LEFT_FOOT_INDEX = <PoseLandmark.LEFT_FOOT_INDEX: 31>\n",
      "     |  \n",
      "     |  LEFT_HEEL = <PoseLandmark.LEFT_HEEL: 29>\n",
      "     |  \n",
      "     |  LEFT_HIP = <PoseLandmark.LEFT_HIP: 23>\n",
      "     |  \n",
      "     |  LEFT_INDEX = <PoseLandmark.LEFT_INDEX: 19>\n",
      "     |  \n",
      "     |  LEFT_KNEE = <PoseLandmark.LEFT_KNEE: 25>\n",
      "     |  \n",
      "     |  LEFT_PINKY = <PoseLandmark.LEFT_PINKY: 17>\n",
      "     |  \n",
      "     |  LEFT_SHOULDER = <PoseLandmark.LEFT_SHOULDER: 11>\n",
      "     |  \n",
      "     |  LEFT_THUMB = <PoseLandmark.LEFT_THUMB: 21>\n",
      "     |  \n",
      "     |  LEFT_WRIST = <PoseLandmark.LEFT_WRIST: 15>\n",
      "     |  \n",
      "     |  MOUTH_LEFT = <PoseLandmark.MOUTH_LEFT: 9>\n",
      "     |  \n",
      "     |  MOUTH_RIGHT = <PoseLandmark.MOUTH_RIGHT: 10>\n",
      "     |  \n",
      "     |  NOSE = <PoseLandmark.NOSE: 0>\n",
      "     |  \n",
      "     |  RIGHT_ANKLE = <PoseLandmark.RIGHT_ANKLE: 28>\n",
      "     |  \n",
      "     |  RIGHT_EAR = <PoseLandmark.RIGHT_EAR: 8>\n",
      "     |  \n",
      "     |  RIGHT_ELBOW = <PoseLandmark.RIGHT_ELBOW: 14>\n",
      "     |  \n",
      "     |  RIGHT_EYE = <PoseLandmark.RIGHT_EYE: 5>\n",
      "     |  \n",
      "     |  RIGHT_EYE_INNER = <PoseLandmark.RIGHT_EYE_INNER: 4>\n",
      "     |  \n",
      "     |  RIGHT_EYE_OUTER = <PoseLandmark.RIGHT_EYE_OUTER: 6>\n",
      "     |  \n",
      "     |  RIGHT_FOOT_INDEX = <PoseLandmark.RIGHT_FOOT_INDEX: 32>\n",
      "     |  \n",
      "     |  RIGHT_HEEL = <PoseLandmark.RIGHT_HEEL: 30>\n",
      "     |  \n",
      "     |  RIGHT_HIP = <PoseLandmark.RIGHT_HIP: 24>\n",
      "     |  \n",
      "     |  RIGHT_INDEX = <PoseLandmark.RIGHT_INDEX: 20>\n",
      "     |  \n",
      "     |  RIGHT_KNEE = <PoseLandmark.RIGHT_KNEE: 26>\n",
      "     |  \n",
      "     |  RIGHT_PINKY = <PoseLandmark.RIGHT_PINKY: 18>\n",
      "     |  \n",
      "     |  RIGHT_SHOULDER = <PoseLandmark.RIGHT_SHOULDER: 12>\n",
      "     |  \n",
      "     |  RIGHT_THUMB = <PoseLandmark.RIGHT_THUMB: 22>\n",
      "     |  \n",
      "     |  RIGHT_WRIST = <PoseLandmark.RIGHT_WRIST: 16>\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from enum.Enum:\n",
      "     |  \n",
      "     |  name\n",
      "     |      The name of the Enum member.\n",
      "     |  \n",
      "     |  value\n",
      "     |      The value of the Enum member.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from enum.EnumMeta:\n",
      "     |  \n",
      "     |  __members__\n",
      "     |      Returns a mapping of member name->value.\n",
      "     |      \n",
      "     |      This mapping lists all enum members, including aliases. Note that this\n",
      "     |      is a read-only view of the internal mapping.\n",
      "\n",
      "DATA\n",
      "    POSE_CONNECTIONS = frozenset({(0, 1), (0, 4), (1, 2), (2, 3), (3, 7), ...\n",
      "\n",
      "FILE\n",
      "    c:\\users\\tawiwut.charuwat\\projects\\airak_vision_pose_detection\\.env\\lib\\site-packages\\mediapipe\\python\\solutions\\pose.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(mp_pose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<enum 'PoseLandmark'>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(mp_pose.PoseLandmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x: 0.30772656202316284\n",
       "y: 0.710430920124054\n",
       "z: -0.004651469178497791\n",
       "visibility: 0.9999597668647766"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "landmarks[mp_pose.PoseLandmark.NOSE.value]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Calculate perceived width and height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.24396061897277832, 0.2551570534706116)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_height_width(landmarks):\n",
    "    '''\n",
    "    Calculate perceived width and height by finding the range between extremes of x amd y coordinates respectively\n",
    "\n",
    "    Params:\n",
    "    landmarks -  pose_landmarks field that contains the pose landmarks (returned from mp.solutions.pose.Pose.process)\n",
    "    '''\n",
    "    joints_x, joints_y = [landmarks[i].x for i in range(0,33)], [landmarks[i].y for i in range(0,33)]\n",
    "    min_x, min_y = min(joints_x), min(joints_y)\n",
    "    max_x, max_y = max(joints_x), max(joints_y)\n",
    "    perceived_width = max_x - min_x\n",
    "    perceived_height = max_y - min_y\n",
    "    return perceived_height, perceived_width\n",
    "def annotate_top_left(frame, height, width):\n",
    "    cv2.putText(\n",
    "        image, f'Height = {height}',\n",
    "        tuple(np.multiply(0.02, frame.shape[:2]).astype(int)), \n",
    "        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,0)\n",
    "        )\n",
    "    cv2.putText(\n",
    "        image, f'Width = {width}',\n",
    "        tuple(np.multiply(0.04, frame.shape[:2]).astype(int)), \n",
    "        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,0)\n",
    "        )\n",
    "\n",
    "calculate_height_width(landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cap = cv2.VideoCapture(0)\n",
    "cap = cv2.VideoCapture('data/50_ways_to_fall.mp4')\n",
    "\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break # use break if capturing a video file, continue if webcam\n",
    "\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) # mediapipe requires RGB\n",
    "        image.flags.writeable = False # helps manage memory\n",
    "\n",
    "        # Make detections\n",
    "        results = pose.process(image)\n",
    "        \n",
    "        # Extract landmarks\n",
    "        try:\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "            height, width = calculate_height_width(landmarks)\n",
    "            annotate_top_left(frame, height, width)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # Render detections\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "        cv2.imshow('Mediapipe Feed', image)\n",
    "        \n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Count falls\n",
    "- when "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_height_width(landmarks):\n",
    "    '''\n",
    "    Calculate perceived width and height by finding the range between extremes of x amd y coordinates respectively\n",
    "\n",
    "    Params:\n",
    "    landmarks -  pose_landmarks field that contains the pose landmarks (returned from mp.solutions.pose.Pose.process)\n",
    "    '''\n",
    "    joints_x, joints_y = [landmarks[i].x for i in range(0,33)], [landmarks[i].y for i in range(0,33)]\n",
    "    min_x, min_y = min(joints_x), min(joints_y)\n",
    "    max_x, max_y = max(joints_x), max(joints_y)\n",
    "    perceived_width = max_x - min_x\n",
    "    perceived_height = max_y - min_y\n",
    "    return perceived_height, perceived_width\n",
    "def annotate_top_left(frame, height, width, fall_count):\n",
    "    cv2.putText(\n",
    "        image, f'Height = {height}',\n",
    "        tuple(np.multiply(0.02, frame.shape[:2]).astype(int)), \n",
    "        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,0)\n",
    "        )\n",
    "    cv2.putText(\n",
    "        image, f'Width = {width}',\n",
    "        tuple(np.multiply(0.04, frame.shape[:2]).astype(int)), \n",
    "        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,0)\n",
    "        )\n",
    "    cv2.putText(\n",
    "        image, f'Fall count = {fall_count}',\n",
    "        tuple(np.multiply(0.06, frame.shape[:2]).astype(int)), \n",
    "        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,0,0)\n",
    "        )\n",
    "def detect_fall(height_0, width_0, height_1, width_1):\n",
    "    '''\n",
    "    Detect a fall when \n",
    "    - width_1 > height_1 (current frame)\n",
    "    and width_0 <= height_0 (previous frame)\n",
    "\n",
    "    Params:\n",
    "    height_0, width_0 - height and width from the previous frame\n",
    "    height_1, width_1 - height and width from the current frame\n",
    "    '''\n",
    "    return width_1 > height_1 and width_0 <= height_0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cap = cv2.VideoCapture(0)\n",
    "cap = cv2.VideoCapture('data/50_ways_to_fall.mp4')\n",
    "\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    # init fall_count and height_0, width_0\n",
    "    fall_count = 0\n",
    "    height_0 = None\n",
    "    width_0 = None\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break # use break if capturing a video file, continue if webcam\n",
    "\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) # mediapipe requires RGB\n",
    "        image.flags.writeable = False # helps manage memory\n",
    "\n",
    "        # Make detections\n",
    "        results = pose.process(image)\n",
    "        \n",
    "        # Extract landmarks\n",
    "        try:\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "            # current frame\n",
    "            height_1, width_1 = calculate_height_width(landmarks)\n",
    "            # detect_fall\n",
    "            if height_0 and width_0:\n",
    "                if detect_fall(height_0, width_0, height_1, width_1):\n",
    "                    fall_count += 1\n",
    "            height_0, width_0 = height_1, width_1\n",
    "            annotate_top_left(frame, height_1, width_1, fall_count)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # Render detections\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "        cv2.imshow('Mediapipe Feed', image)\n",
    "        \n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- able to detect simple singular falls well\n",
    "- overcount rapid movements\n",
    "- ...... note some more with ting ..."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "970b4d8da43189f768a3799d2b7381cf5f8708f8abc1dd3fa9ac3f3dbcc81f1b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('.env': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
